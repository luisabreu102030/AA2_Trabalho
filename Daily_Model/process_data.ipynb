{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3532bd52",
   "metadata": {},
   "source": [
    "# Processamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ab684",
   "metadata": {},
   "source": [
    "Este notebook encontra-se dividido em três partes. A primeira parte contém todo o processamento de dados efectuado aos dados recolhidos para a construção do dataset daily_covid, e asegunda contém todos os passos aplicados na construção do dataset daily_diabetes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1dc22",
   "metadata": {},
   "source": [
    "Para o processamento dos diferentes datasets foram utilizadas as bibliotecas python: numpy e pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4858809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6c82f",
   "metadata": {},
   "source": [
    "Devido aos diferentes sinais separadores de atributos dos registos presentes nos datasets utilizados, foi necessário criar funções diferentes para o load dos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af81ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normal_dataset(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0249b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(path):\n",
    "    return pd.read_csv(path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a6e2d",
   "metadata": {},
   "source": [
    "O método to_csv_file, é utilizado para construir o dataset final que posteriormente será utilizado nas previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28904f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv_file(df, file_name):\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faff599",
   "metadata": {},
   "source": [
    "## Parte 1 - Daily_covid dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa5ceb1",
   "metadata": {},
   "source": [
    "Nesta Parte, irei pormenorizar todos as escolhas e passos tomados, para que fosse possível construir um dataset final, capaz de ser robusto o suficiente para eventualmente se obter uma boa previsão para o número de mortes causadas por covid-19 em Portugal. Para a construção deste dataset foram utilizados dados obtidos no repositório [Data Science for Social Good Portugal(DDSGP)](https://github.com/dssg-pt/covid19pt-data), que correspondem a registos diários de mortes por covid e dados atmosféricos obtidos através da plataforma [Visualcrossing](https://www.visualcrossing.com/weather/weather-data-services#/login). Estes dados atmosféricos são as médias diárias para Portugal no mesmo periodo de registos observados no dataset do repositório DSSGP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51750feb",
   "metadata": {},
   "source": [
    "### Load do dataset daily_covid_19_portugal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e4f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_covid = load_normal_dataset(\"daily_datasets/daily_covid_19_portugal.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20294f",
   "metadata": {},
   "source": [
    "### Load dos datasets atmosféricos de Portugal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c8f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_temp_2020_01 = load_normal_dataset('daily_datasets/temp_portugal_01012020_31032020.csv')\n",
    "df_raw_temp_2020_02 = load_normal_dataset('daily_datasets/temp_portugal_01042020_30062020.csv')\n",
    "df_raw_temp_2020_03 = load_normal_dataset('daily_datasets/temp_portugal_01072020_30092020.csv')\n",
    "df_raw_temp_2020_04 = load_normal_dataset('daily_datasets/temp_portugal_01102020_31122020.csv')\n",
    "df_raw_temp_2020_05 = load_normal_dataset('daily_datasets/temp_portugal_01012021_31032021.csv')\n",
    "df_raw_temp_2020_06 = load_normal_dataset('daily_datasets/temp_portugal_042021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0f2e8",
   "metadata": {},
   "source": [
    "Após estes loads, foi necessário efetuar tratamento de dados obtidos, nomeadamente tratamento de missing values, alterações dos tipos dos atributos e por fim a concatenação dos diferentes datasets num só dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919886d9",
   "metadata": {},
   "source": [
    "### Tratamento dos datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c4077",
   "metadata": {},
   "source": [
    "Em prepare_data_covid, encontra-se todo o tratamento efectuado ao dataset daily_covid_19_portugal. Nesta função foram eliminados atributos cujas colunas apresentavam número de missing válues superiores a 20% da totalidade dos dados. Eliminamos a diferenciação do número de óbitos e infectados por género, para isso somou-se o valor dos atributos correspondentes à mesma idade, resultando em apenas uma coluna correspondente à sua respetiva idade (ex: obitos_20_29m + obitos_20_29f = obitos_20_29, infetados_20_29m + infetados_20_29f = infetados_20_29). Atributos cujo o número de missing values era inferior a 20%, esses missing values foram substituídos pelo valor registado no dia anterior (para missing values referentes a registos não iniciais da série temporal), ou foram substituídos pelo valor 0 (para missing values referentes a registos no periodo inicial da pandemia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ed831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_covid(df_raw_covid):\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df_aux = df_raw_covid.copy()\n",
    "    print(df_aux.shape)\n",
    "\n",
    "    print(\"Number of missing Values: \")\n",
    "    print(df_aux.isnull().sum())\n",
    "\n",
    "\n",
    "    df_aux = df_raw_covid.drop(columns=['data_dados', 'confirmados_estrangeiro', 'lab', 'suspeitos', 'vigilancia',\n",
    "                                        'n_confirmados', 'cadeias_transmissao', 'transmissao_importada',\n",
    "                                        'sintomas_tosse', 'sintomas_febre', 'sintomas_cefaleia',\n",
    "                                        'sintomas_dores_musculares', 'sintomas_dificuldade_respiratoria', 'sintomas_fraqueza_generalizada',\n",
    "                                        'obitos_estrangeiro', 'recuperados_arsnorte','recuperados_arscentro',\n",
    "                                        'recuperados_arslvt','recuperados_arsalentejo',\n",
    "                                        'recuperados_arsalgarve','recuperados_acores', 'recuperados_madeira',\n",
    "                                        'recuperados_estrangeiro','confirmados_desconhecidos_m','confirmados_desconhecidos_f',\n",
    "                                        'incidencia_nacional','incidencia_continente','rt_nacional', 'rt_continente',\n",
    "                                        'obitos_f', 'obitos_m','confirmados_desconhecidos','confirmados_f','confirmados_m',\n",
    "                                        'internados'], inplace=False)\n",
    "\n",
    "\n",
    "    #somar colunas casos confirmados com mesma faixa etaria ignorando sexo\n",
    "    df_aux['confirmados_0_9'] = df_aux.apply(lambda x: x['confirmados_0_9_f'] + x['confirmados_0_9_m'], axis=1)\n",
    "    df_aux['confirmados_10_19'] = df_aux.apply(lambda x: x['confirmados_10_19_f'] + x['confirmados_10_19_m'], axis=1)\n",
    "    df_aux['confirmados_20_29'] = df_aux.apply(lambda x: x['confirmados_20_29_f'] + x['confirmados_20_29_m'], axis=1)\n",
    "    df_aux['confirmados_30_39'] = df_aux.apply(lambda x: x['confirmados_30_39_f'] + x['confirmados_30_39_m'], axis=1)\n",
    "    df_aux['confirmados_40_49'] = df_aux.apply(lambda x: x['confirmados_40_49_f'] + x['confirmados_40_49_m'], axis=1)\n",
    "    df_aux['confirmados_50_59'] = df_aux.apply(lambda x: x['confirmados_50_59_f'] + x['confirmados_50_59_m'], axis=1)\n",
    "    df_aux['confirmados_60_69'] = df_aux.apply(lambda x: x['confirmados_60_69_f'] + x['confirmados_60_69_m'], axis=1)\n",
    "    df_aux['confirmados_70_79'] = df_aux.apply(lambda x: x['confirmados_70_79_f'] + x['confirmados_70_79_m'], axis=1)\n",
    "    df_aux['confirmados_80_plus'] = df_aux.apply(lambda x: x['confirmados_80_plus_f'] + x['confirmados_80_plus_m'], axis=1)\n",
    "\n",
    "    df_aux = df_aux.drop(columns=['confirmados_0_9_f','confirmados_0_9_m',\n",
    "                            'confirmados_10_19_f','confirmados_10_19_m',\n",
    "                            'confirmados_20_29_f','confirmados_20_29_m',\n",
    "                            'confirmados_30_39_f','confirmados_30_39_m',\n",
    "                            'confirmados_40_49_f','confirmados_40_49_m',\n",
    "                            'confirmados_50_59_f','confirmados_50_59_m',\n",
    "                            'confirmados_60_69_f','confirmados_60_69_m',\n",
    "                            'confirmados_70_79_f','confirmados_70_79_m',\n",
    "                            'confirmados_80_plus_f','confirmados_80_plus_m'], inplace=False)\n",
    "\n",
    "\n",
    "    #somar colunas obitos confirmados com mesma faixa etaria ignorando sexo\n",
    "    df_aux['obitos_0_9'] = df_aux.apply(lambda x: x['obitos_0_9_f'] + x['obitos_0_9_m'], axis=1)\n",
    "    df_aux['obitos_10_19'] = df_aux.apply(lambda x: x['obitos_10_19_f'] + x['obitos_10_19_m'], axis=1)\n",
    "    df_aux['obitos_20_29'] = df_aux.apply(lambda x: x['obitos_20_29_f'] + x['obitos_20_29_m'], axis=1)\n",
    "    df_aux['obitos_30_39'] = df_aux.apply(lambda x: x['obitos_30_39_f'] + x['obitos_30_39_m'], axis=1)\n",
    "    df_aux['obitos_40_49'] = df_aux.apply(lambda x: x['obitos_40_49_f'] + x['obitos_40_49_m'], axis=1)\n",
    "    df_aux['obitos_50_59'] = df_aux.apply(lambda x: x['obitos_50_59_f'] + x['obitos_50_59_m'], axis=1)\n",
    "    df_aux['obitos_60_69'] = df_aux.apply(lambda x: x['obitos_60_69_f'] + x['obitos_60_69_m'], axis=1)\n",
    "    df_aux['obitos_70_79'] = df_aux.apply(lambda x: x['obitos_70_79_f'] + x['obitos_70_79_m'], axis=1)\n",
    "    df_aux['obitos_80_plus'] = df_aux.apply(lambda x: x['obitos_80_plus_f'] + x['obitos_80_plus_m'], axis=1)\n",
    "\n",
    "    df_aux = df_aux.drop(columns=['obitos_0_9_f','obitos_0_9_m',\n",
    "                            'obitos_10_19_f','obitos_10_19_m',\n",
    "                            'obitos_20_29_f','obitos_20_29_m',\n",
    "                            'obitos_30_39_f','obitos_30_39_m',\n",
    "                            'obitos_40_49_f','obitos_40_49_m',\n",
    "                            'obitos_50_59_f','obitos_50_59_m',\n",
    "                            'obitos_60_69_f','obitos_60_69_m',\n",
    "                            'obitos_70_79_f','obitos_70_79_m',\n",
    "                            'obitos_80_plus_f','obitos_80_plus_m'], inplace=False)\n",
    "\n",
    "    #drop das ultimas 2 rows do dataset\n",
    "    df_aux = df_aux.drop(df_aux.tail(2).index)\n",
    "\n",
    "    #fill row com missing values aplicando ffill method, n vejo problema pois aind a não haviam casos e faltando um dia mantem o valor do dia anterio\n",
    "    #obitos_80_plus\n",
    "    df_aux = df_aux.fillna(method='ffill', limit=1)\n",
    "    #obitos_arsalentejo\n",
    "    df_aux = df_aux.fillna(method='ffill', limit=19)\n",
    "    #como os nan estão ao inicio da pandemia, é natural n terem valor, logo vou substituir esses NaN por 0\n",
    "    df_aux = df_aux.fillna(0)\n",
    "\n",
    "    #print(df_aux.isnull().sum())\n",
    "    #print(df_aux)\n",
    "    #print(\"Number of missing Values: \")\n",
    "    #print(df_aux.isnull().sum())\n",
    "    # counting the duplicates\n",
    "    dups = df_aux.pivot_table(index=['data','confirmados'], aggfunc='size')\n",
    "    # displaying the duplicate Series\n",
    "    print(dups)\n",
    "    #Não tem duplicados\n",
    "\n",
    "\n",
    "    df_aux.columns = ['Date', 'confirmados', 'confirmados_arsnorte', 'confirmados_arscentro',\n",
    "     'confirmados_arslvt', 'confirmados_arsalentejo',\n",
    "     'confirmados_arsalgarve', 'confirmados_acores', 'confirmados_madeira',\n",
    "     'confirmados_novos', 'recuperados', 'obitos', 'internados_uci',\n",
    "     'obitos_arsnorte', 'obitos_arscentro', 'obitos_arslvt',\n",
    "     'obitos_arsalentejo', 'obitos_arsalgarve', 'obitos_acores',\n",
    "     'obitos_madeira', 'ativos', 'internados_enfermaria', 'confirmados_0_9',\n",
    "     'confirmados_10_19', 'confirmados_20_29', 'confirmados_30_39',\n",
    "     'confirmados_40_49', 'confirmados_50_59', 'confirmados_60_69',\n",
    "     'confirmados_70_79', 'confirmados_80_plus', 'obitos_0_9',\n",
    "     'obitos_10_19', 'obitos_20_29', 'obitos_30_39', 'obitos_40_49',\n",
    "     'obitos_50_59', 'obitos_60_69', 'obitos_70_79', 'obitos_80_plus']\n",
    "\n",
    "    #Colocar a diferença em vezz de valor acumulado nos dias.\n",
    "    df_aux[\"confirmados\"] = df_aux.loc[:,[\"confirmados\"]].diff()\n",
    "    df_aux[\"confirmados_arsnorte\"] = df_aux.loc[:,[\"confirmados_arsnorte\"]].diff()\n",
    "    df_aux[\"confirmados_arscentro\"] = df_aux.loc[:,[\"confirmados_arscentro\"]].diff()\n",
    "    df_aux[\"confirmados_arslvt\"] = df_aux.loc[:,[\"confirmados_arslvt\"]].diff()\n",
    "    df_aux[\"confirmados_arsalentejo\"] = df_aux.loc[:,[\"confirmados_arsalentejo\"]].diff()\n",
    "    df_aux[\"confirmados_arsalgarve\"] = df_aux.loc[:,[\"confirmados_arsalgarve\"]].diff()\n",
    "    df_aux[\"confirmados_acores\"] = df_aux.loc[:,[\"confirmados_acores\"]].diff()\n",
    "    df_aux[\"confirmados_madeira\"] = df_aux.loc[:,[\"confirmados_madeira\"]].diff()\n",
    "    df_aux[\"recuperados\"] = df_aux.loc[:,[\"recuperados\"]].diff()\n",
    "    df_aux[\"obitos\"] = df_aux.loc[:,[\"obitos\"]].diff()\n",
    "    df_aux[\"internados_uci\"] = df_aux.loc[:,[\"internados_uci\"]].diff()\n",
    "    df_aux[\"obitos_arsnorte\"] = df_aux.loc[:,[\"obitos_arsnorte\"]].diff()\n",
    "    df_aux[\"obitos_arscentro\"] = df_aux.loc[:,[\"obitos_arscentro\"]].diff()\n",
    "    df_aux[\"obitos_arslvt\"] = df_aux.loc[:,[\"obitos_arslvt\"]].diff()\n",
    "    df_aux[\"obitos_arsalentejo\"] = df_aux.loc[:,[\"obitos_arsalentejo\"]].diff()\n",
    "    df_aux[\"obitos_arsalgarve\"] = df_aux.loc[:,[\"obitos_arsalgarve\"]].diff()\n",
    "    df_aux[\"obitos_acores\"] = df_aux.loc[:,[\"obitos_acores\"]].diff()\n",
    "    df_aux[\"obitos_madeira\"] = df_aux.loc[:,[\"obitos_madeira\"]].diff()\n",
    "    df_aux[\"ativos\"] = df_aux.loc[:,[\"ativos\"]].diff()\n",
    "    df_aux[\"internados_enfermaria\"] = df_aux.loc[:,[\"internados_enfermaria\"]].diff()\n",
    "    df_aux[\"confirmados_0_9\"] = df_aux.loc[:,[\"confirmados_0_9\"]].diff()\n",
    "    df_aux[\"confirmados_10_19\"] = df_aux.loc[:,[\"confirmados_10_19\"]].diff()\n",
    "    df_aux[\"confirmados_20_29\"] = df_aux.loc[:,[\"confirmados_20_29\"]].diff()\n",
    "    df_aux[\"confirmados_30_39\"] = df_aux.loc[:,[\"confirmados_30_39\"]].diff()\n",
    "    df_aux[\"confirmados_40_49\"] = df_aux.loc[:,[\"confirmados_40_49\"]].diff()\n",
    "    df_aux[\"confirmados_50_59\"] = df_aux.loc[:,[\"confirmados_50_59\"]].diff()\n",
    "    df_aux[\"confirmados_60_69\"] = df_aux.loc[:,[\"confirmados_60_69\"]].diff()\n",
    "    df_aux[\"confirmados_70_79\"] = df_aux.loc[:,[\"confirmados_70_79\"]].diff()\n",
    "    df_aux[\"confirmados_80_plus\"] = df_aux.loc[:,[\"confirmados_80_plus\"]].diff()\n",
    "    df_aux[\"obitos_0_9\"] = df_aux.loc[:,[\"obitos_0_9\"]].diff()\n",
    "    df_aux[\"obitos_10_19\"] = df_aux.loc[:,[\"obitos_10_19\"]].diff()\n",
    "    df_aux[\"obitos_20_29\"] = df_aux.loc[:,[\"obitos_20_29\"]].diff()\n",
    "    df_aux[\"obitos_30_39\"] = df_aux.loc[:,[\"obitos_30_39\"]].diff()\n",
    "    df_aux[\"obitos_40_49\"] = df_aux.loc[:,[\"obitos_40_49\"]].diff()\n",
    "    df_aux[\"obitos_50_59\"] = df_aux.loc[:,[\"obitos_50_59\"]].diff()\n",
    "    df_aux[\"obitos_60_69\"] = df_aux.loc[:,[\"obitos_60_69\"]].diff()\n",
    "    df_aux[\"obitos_70_79\"] = df_aux.loc[:,[\"obitos_70_79\"]].diff()\n",
    "    df_aux[\"obitos_80_plus\"] = df_aux.loc[:,[\"obitos_80_plus\"]].diff()\n",
    "    \n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    #pd.set_option('display.max_columns', None)\n",
    "    #print(df_aux.columns)\n",
    "    #print(df_aux)\n",
    "\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45898054",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 93)\n",
      "Number of missing Values: \n",
      "data                                   0\n",
      "data_dados                             0\n",
      "confirmados                            0\n",
      "confirmados_arsnorte                   0\n",
      "confirmados_arscentro                  0\n",
      "confirmados_arslvt                     0\n",
      "confirmados_arsalentejo                0\n",
      "confirmados_arsalgarve                 0\n",
      "confirmados_acores                     0\n",
      "confirmados_madeira                    0\n",
      "confirmados_estrangeiro              408\n",
      "confirmados_novos                      0\n",
      "recuperados                            0\n",
      "obitos                                 0\n",
      "internados                             8\n",
      "internados_uci                        17\n",
      "lab                                  260\n",
      "suspeitos                            251\n",
      "vigilancia                             7\n",
      "n_confirmados                        269\n",
      "cadeias_transmissao                  409\n",
      "transmissao_importada                257\n",
      "confirmados_0_9_f                      8\n",
      "confirmados_0_9_m                      8\n",
      "confirmados_10_19_f                    8\n",
      "confirmados_10_19_m                    8\n",
      "confirmados_20_29_f                    8\n",
      "confirmados_20_29_m                    8\n",
      "confirmados_30_39_f                    8\n",
      "confirmados_30_39_m                    8\n",
      "confirmados_40_49_f                    8\n",
      "confirmados_40_49_m                    8\n",
      "confirmados_50_59_f                    8\n",
      "confirmados_50_59_m                    8\n",
      "confirmados_60_69_f                    8\n",
      "confirmados_60_69_m                    8\n",
      "confirmados_70_79_f                    8\n",
      "confirmados_70_79_m                    8\n",
      "confirmados_80_plus_f                  8\n",
      "confirmados_80_plus_m                  8\n",
      "sintomas_tosse                       257\n",
      "sintomas_febre                       257\n",
      "sintomas_dificuldade_respiratoria    259\n",
      "sintomas_cefaleia                    257\n",
      "sintomas_dores_musculares            257\n",
      "sintomas_fraqueza_generalizada       257\n",
      "confirmados_f                         22\n",
      "confirmados_m                         22\n",
      "obitos_arsnorte                        0\n",
      "obitos_arscentro                       0\n",
      "obitos_arslvt                          0\n",
      "obitos_arsalentejo                     0\n",
      "obitos_arsalgarve                      0\n",
      "obitos_acores                          0\n",
      "obitos_madeira                         0\n",
      "obitos_estrangeiro                   393\n",
      "recuperados_arsnorte                 414\n",
      "recuperados_arscentro                414\n",
      "recuperados_arslvt                   414\n",
      "recuperados_arsalentejo              414\n",
      "recuperados_arsalgarve               414\n",
      "recuperados_acores                   414\n",
      "recuperados_madeira                  414\n",
      "recuperados_estrangeiro              414\n",
      "obitos_0_9_f                          29\n",
      "obitos_0_9_m                          29\n",
      "obitos_10_19_f                        29\n",
      "obitos_10_19_m                        29\n",
      "obitos_20_29_f                        29\n",
      "obitos_20_29_m                        29\n",
      "obitos_30_39_f                        29\n",
      "obitos_30_39_m                        29\n",
      "obitos_40_49_f                        29\n",
      "obitos_40_49_m                        29\n",
      "obitos_50_59_f                        29\n",
      "obitos_50_59_m                        29\n",
      "obitos_60_69_f                        30\n",
      "obitos_60_69_m                        30\n",
      "obitos_70_79_f                        30\n",
      "obitos_70_79_m                        30\n",
      "obitos_80_plus_f                      30\n",
      "obitos_80_plus_m                      30\n",
      "obitos_f                              27\n",
      "obitos_m                              27\n",
      "confirmados_desconhecidos_m          348\n",
      "confirmados_desconhecidos_f          348\n",
      "ativos                                 5\n",
      "internados_enfermaria                  5\n",
      "confirmados_desconhecidos             56\n",
      "incidencia_nacional                  383\n",
      "incidencia_continente                383\n",
      "rt_nacional                          383\n",
      "rt_continente                        383\n",
      "dtype: int64\n",
      "data        confirmados\n",
      "01-01-2021  420629         1\n",
      "01-02-2021  726321         1\n",
      "01-03-2020  0              1\n",
      "01-03-2021  804956         1\n",
      "01-04-2020  8251           1\n",
      "01-04-2021  822314         1\n",
      "01-05-2020  24987          1\n",
      "01-06-2020  32700          1\n",
      "01-07-2020  42523          1\n",
      "01-08-2020  51310          1\n",
      "01-09-2020  58243          1\n",
      "01-10-2020  76396          1\n",
      "01-11-2020  144341         1\n",
      "01-12-2020  300462         1\n",
      "02-01-2021  423870         1\n",
      "02-02-2021  731861         1\n",
      "02-03-2020  2              1\n",
      "02-03-2021  805647         1\n",
      "02-04-2020  9034           1\n",
      "02-04-2021  822862         1\n",
      "02-05-2020  25190          1\n",
      "02-06-2020  32895          1\n",
      "02-07-2020  42901          1\n",
      "02-08-2020  51463          1\n",
      "02-09-2020  58633          1\n",
      "02-10-2020  77284          1\n",
      "02-11-2020  146847         1\n",
      "02-12-2020  303846         1\n",
      "03-01-2021  427254         1\n",
      "03-02-2021  740944         1\n",
      "03-03-2020  4              1\n",
      "03-03-2021  806626         1\n",
      "03-04-2020  9886           1\n",
      "03-04-2021  823142         1\n",
      "03-05-2020  25282          1\n",
      "03-06-2020  33261          1\n",
      "03-07-2020  43356          1\n",
      "03-08-2020  51569          1\n",
      "03-09-2020  59051          1\n",
      "03-10-2020  78247          1\n",
      "03-11-2020  149443         1\n",
      "03-12-2020  307618         1\n",
      "04-01-2021  431623         1\n",
      "04-02-2021  748858         1\n",
      "04-03-2020  6              1\n",
      "04-03-2021  807456         1\n",
      "04-04-2020  10524          1\n",
      "04-04-2021  823335         1\n",
      "04-05-2020  25524          1\n",
      "04-06-2020  33592          1\n",
      "04-07-2020  43769          1\n",
      "04-08-2020  51681          1\n",
      "04-09-2020  59457          1\n",
      "04-10-2020  79151          1\n",
      "04-11-2020  156940         1\n",
      "04-12-2020  312553         1\n",
      "05-01-2021  436579         1\n",
      "05-02-2021  755774         1\n",
      "05-03-2020  9              1\n",
      "05-03-2021  808405         1\n",
      "05-04-2020  11278          1\n",
      "05-04-2021  823494         1\n",
      "05-05-2020  25702          1\n",
      "05-06-2020  33969          1\n",
      "05-07-2020  44097          1\n",
      "05-08-2020  51848          1\n",
      "05-09-2020  59943          1\n",
      "05-10-2020  79885          1\n",
      "05-11-2020  161350         1\n",
      "05-12-2020  318640         1\n",
      "06-01-2021  446606         1\n",
      "06-02-2021  761906         1\n",
      "06-03-2020  13             1\n",
      "06-03-2021  809412         1\n",
      "06-04-2020  11730          1\n",
      "06-04-2021  824368         1\n",
      "06-05-2020  26182          1\n",
      "06-06-2020  34351          1\n",
      "06-07-2020  44329          1\n",
      "06-08-2020  52061          1\n",
      "06-09-2020  60258          1\n",
      "06-10-2020  80312          1\n",
      "06-11-2020  166900         1\n",
      "06-12-2020  322474         1\n",
      "07-01-2021  456533         1\n",
      "07-02-2021  765414         1\n",
      "07-03-2020  21             1\n",
      "07-03-2021  810094         1\n",
      "07-04-2020  12442          1\n",
      "07-04-2021  825031         1\n",
      "07-05-2020  26715          1\n",
      "07-06-2020  34693          1\n",
      "07-07-2020  44616          1\n",
      "07-08-2020  52351          1\n",
      "07-09-2020  60507          1\n",
      "07-10-2020  81256          1\n",
      "07-11-2020  173540         1\n",
      "07-12-2020  325071         1\n",
      "08-01-2021  466709         1\n",
      "08-02-2021  767919         1\n",
      "08-03-2020  30             1\n",
      "08-03-2021  810459         1\n",
      "08-04-2020  13141          1\n",
      "08-04-2021  825633         1\n",
      "08-05-2020  27268          1\n",
      "08-06-2020  34885          1\n",
      "08-07-2020  45059          1\n",
      "08-08-2020  52537          1\n",
      "08-09-2020  60895          1\n",
      "08-10-2020  82534          1\n",
      "08-11-2020  179324         1\n",
      "08-12-2020  327976         1\n",
      "09-01-2021  476187         1\n",
      "09-02-2021  770502         1\n",
      "09-03-2020  39             1\n",
      "09-03-2021  811306         1\n",
      "09-04-2020  13956          1\n",
      "09-04-2021  826327         1\n",
      "09-05-2020  27406          1\n",
      "09-06-2020  35306          1\n",
      "09-07-2020  45477          1\n",
      "09-08-2020  52668          1\n",
      "09-09-2020  61541          1\n",
      "09-10-2020  83928          1\n",
      "09-11-2020  183420         1\n",
      "09-12-2020  332073         1\n",
      "10-01-2021  483689         1\n",
      "10-02-2021  774889         1\n",
      "10-03-2020  41             1\n",
      "10-03-2021  811948         1\n",
      "10-04-2020  15472          1\n",
      "10-04-2021  826928         1\n",
      "10-05-2020  27581          1\n",
      "10-06-2020  35600          1\n",
      "10-07-2020  45879          1\n",
      "10-08-2020  52825          1\n",
      "10-09-2020  62126          1\n",
      "10-10-2020  85574          1\n",
      "10-11-2020  187237         1\n",
      "10-12-2020  335207         1\n",
      "11-01-2021  489293         1\n",
      "11-02-2021  778369         1\n",
      "11-03-2020  59             1\n",
      "11-03-2021  812575         1\n",
      "11-04-2020  15987          1\n",
      "11-04-2021  827494         1\n",
      "11-05-2020  27679          1\n",
      "11-06-2020  35910          1\n",
      "11-07-2020  46221          1\n",
      "11-08-2020  52945          1\n",
      "11-09-2020  62813          1\n",
      "11-10-2020  86664          1\n",
      "11-11-2020  192172         1\n",
      "11-12-2020  340287         1\n",
      "12-01-2021  496552         1\n",
      "12-02-2021  781223         1\n",
      "12-03-2020  78             1\n",
      "12-03-2021  813152         1\n",
      "12-04-2020  16585          1\n",
      "12-04-2021  827765         1\n",
      "12-05-2020  27913          1\n",
      "12-06-2020  36180          1\n",
      "12-07-2020  46512          1\n",
      "12-08-2020  53223          1\n",
      "12-09-2020  63310          1\n",
      "12-10-2020  87913          1\n",
      "12-11-2020  198011         1\n",
      "12-12-2020  344700         1\n",
      "13-01-2021  507108         1\n",
      "13-02-2021  784079         1\n",
      "13-03-2020  112            1\n",
      "13-03-2021  813716         1\n",
      "13-04-2020  16934          1\n",
      "13-04-2021  828173         1\n",
      "13-05-2020  28132          1\n",
      "13-06-2020  36463          1\n",
      "13-07-2020  46818          1\n",
      "13-08-2020  53548          1\n",
      "13-09-2020  63983          1\n",
      "13-10-2020  89121          1\n",
      "13-11-2020  204664         1\n",
      "13-12-2020  348744         1\n",
      "14-01-2021  517806         1\n",
      "14-02-2021  785756         1\n",
      "14-03-2020  169            1\n",
      "14-03-2021  814257         1\n",
      "14-04-2020  17448          1\n",
      "14-04-2021  828857         1\n",
      "14-05-2020  28319          1\n",
      "14-06-2020  36690          1\n",
      "14-07-2020  47051          1\n",
      "14-08-2020  53783          1\n",
      "14-09-2020  64596          1\n",
      "14-10-2020  91193          1\n",
      "14-11-2020  211266         1\n",
      "14-12-2020  350938         1\n",
      "15-01-2021  528469         1\n",
      "15-02-2021  787059         1\n",
      "15-03-2020  245            1\n",
      "15-03-2021  814513         1\n",
      "15-04-2020  18091          1\n",
      "15-04-2021  829358         1\n",
      "15-05-2020  28583          1\n",
      "15-06-2020  37036          1\n",
      "15-07-2020  47426          1\n",
      "15-08-2020  53981          1\n",
      "15-09-2020  65021          1\n",
      "15-10-2020  93294          1\n",
      "15-11-2020  217301         1\n",
      "15-12-2020  353576         1\n",
      "16-01-2021  539416         1\n",
      "16-02-2021  788561         1\n",
      "16-03-2020  331            1\n",
      "16-03-2021  814897         1\n",
      "16-04-2020  18841          1\n",
      "16-04-2021  829911         1\n",
      "16-05-2020  28810          1\n",
      "16-06-2020  37336          1\n",
      "16-07-2020  47765          1\n",
      "16-08-2020  54102          1\n",
      "16-09-2020  65626          1\n",
      "16-10-2020  95902          1\n",
      "16-11-2020  225672         1\n",
      "16-12-2020  358296         1\n",
      "17-01-2021  549801         1\n",
      "17-02-2021  790885         1\n",
      "17-03-2020  448            1\n",
      "17-03-2021  815570         1\n",
      "17-04-2020  19022          1\n",
      "17-04-2021  830560         1\n",
      "17-05-2020  29036          1\n",
      "17-06-2020  37672          1\n",
      "17-07-2020  48077          1\n",
      "17-08-2020  54234          1\n",
      "17-09-2020  66396          1\n",
      "17-10-2020  98055          1\n",
      "17-11-2020  230124         1\n",
      "17-12-2020  362616         1\n",
      "18-01-2021  556503         1\n",
      "18-02-2021  792829         1\n",
      "18-03-2020  642            1\n",
      "18-03-2021  816055         1\n",
      "18-04-2020  19685          1\n",
      "18-04-2021  831001         1\n",
      "18-05-2020  29209          1\n",
      "18-06-2020  38089          1\n",
      "18-07-2020  48390          1\n",
      "18-08-2020  54448          1\n",
      "18-09-2020  67176          1\n",
      "18-10-2020  99911          1\n",
      "18-11-2020  236015         1\n",
      "18-12-2020  366952         1\n",
      "19-01-2021  566958         1\n",
      "19-02-2021  794769         1\n",
      "19-03-2020  785            1\n",
      "19-03-2021  816623         1\n",
      "19-04-2020  20206          1\n",
      "19-04-2021  831221         1\n",
      "19-05-2020  29432          1\n",
      "19-06-2020  38464          1\n",
      "19-07-2020  48636          1\n",
      "19-08-2020  54701          1\n",
      "19-09-2020  68025          1\n",
      "19-10-2020  101860         1\n",
      "19-11-2020  243009         1\n",
      "19-12-2020  370787         1\n",
      "20-01-2021  581605         1\n",
      "20-02-2021  796339         1\n",
      "20-03-2020  1020           1\n",
      "20-03-2021  817080         1\n",
      "20-04-2020  20863          1\n",
      "20-04-2021  831645         1\n",
      "20-05-2020  29660          1\n",
      "20-06-2020  38841          1\n",
      "20-07-2020  48771          1\n",
      "20-08-2020  54992          1\n",
      "20-09-2020  68577          1\n",
      "20-10-2020  103736         1\n",
      "20-11-2020  249498         1\n",
      "20-12-2020  374121         1\n",
      "21-01-2021  595149         1\n",
      "21-02-2021  797525         1\n",
      "21-03-2020  1280           1\n",
      "21-03-2021  817530         1\n",
      "21-04-2020  21379          1\n",
      "21-04-2021  832255         1\n",
      "21-05-2020  29912          1\n",
      "21-06-2020  39133          1\n",
      "21-07-2020  48898          1\n",
      "21-08-2020  55211          1\n",
      "21-09-2020  69200          1\n",
      "21-10-2020  106271         1\n",
      "21-11-2020  255970         1\n",
      "21-12-2020  376220         1\n",
      "22-01-2021  609136         1\n",
      "22-02-2021  798074         1\n",
      "22-03-2020  1600           1\n",
      "22-03-2021  817778         1\n",
      "22-04-2020  21982          1\n",
      "22-04-2021  832891         1\n",
      "22-05-2020  30200          1\n",
      "22-06-2020  39392          1\n",
      "22-07-2020  49150          1\n",
      "22-08-2020  55452          1\n",
      "22-09-2020  69663          1\n",
      "22-10-2020  109541         1\n",
      "22-11-2020  260758         1\n",
      "22-12-2020  378656         1\n",
      "23-01-2021  624469         1\n",
      "23-02-2021  799106         1\n",
      "23-03-2020  2060           1\n",
      "23-03-2021  818212         1\n",
      "23-04-2020  22353          1\n",
      "23-05-2020  30471          1\n",
      "23-06-2020  39737          1\n",
      "23-07-2020  49379          1\n",
      "23-08-2020  55597          1\n",
      "23-09-2020  70465          1\n",
      "23-10-2020  112440         1\n",
      "23-11-2020  264802         1\n",
      "23-12-2020  383258         1\n",
      "24-01-2021  636190         1\n",
      "24-02-2021  800586         1\n",
      "24-03-2020  2362           1\n",
      "24-03-2021  818787         1\n",
      "24-04-2020  22797          1\n",
      "24-05-2020  30623          1\n",
      "24-06-2020  40104          1\n",
      "24-07-2020  49692          1\n",
      "24-08-2020  55720          1\n",
      "24-09-2020  71156          1\n",
      "24-10-2020  116109         1\n",
      "24-11-2020  268721         1\n",
      "24-12-2020  387636         1\n",
      "25-01-2021  643113         1\n",
      "25-02-2021  801746         1\n",
      "25-03-2020  2995           1\n",
      "25-03-2021  819210         1\n",
      "25-04-2020  23271          1\n",
      "25-05-2020  30788          1\n",
      "25-06-2020  40415          1\n",
      "25-07-2020  49955          1\n",
      "25-08-2020  55912          1\n",
      "25-09-2020  72055          1\n",
      "25-10-2020  118686         1\n",
      "25-11-2020  274011         1\n",
      "25-12-2020  391782         1\n",
      "26-01-2021  653878         1\n",
      "26-02-2020  0              1\n",
      "26-02-2021  802773         1\n",
      "26-03-2020  3544           1\n",
      "26-03-2021  819698         1\n",
      "26-04-2020  23683          1\n",
      "26-05-2020  31007          1\n",
      "26-06-2020  40866          1\n",
      "26-07-2020  50164          1\n",
      "26-08-2020  56274          1\n",
      "26-09-2020  72939          1\n",
      "26-10-2020  121133         1\n",
      "26-11-2020  280394         1\n",
      "26-12-2020  392996         1\n",
      "27-01-2021  668951         1\n",
      "27-02-2020  0              1\n",
      "27-02-2021  803844         1\n",
      "27-03-2020  4268           1\n",
      "27-03-2021  820042         1\n",
      "27-04-2020  23846          1\n",
      "27-05-2020  31292          1\n",
      "27-06-2020  41189          1\n",
      "27-07-2020  50299          1\n",
      "27-08-2020  56671          1\n",
      "27-09-2020  73604          1\n",
      "27-10-2020  124432         1\n",
      "27-11-2020  285838         1\n",
      "27-12-2020  394573         1\n",
      "28-01-2021  685383         1\n",
      "28-02-2020  0              1\n",
      "28-02-2021  804562         1\n",
      "28-03-2020  5170           1\n",
      "28-03-2021  820407         1\n",
      "28-04-2020  24141          1\n",
      "28-05-2020  31596          1\n",
      "28-06-2020  41646          1\n",
      "28-07-2020  50410          1\n",
      "28-08-2020  57074          1\n",
      "28-09-2020  74029          1\n",
      "28-10-2020  128392         1\n",
      "28-11-2020  290706         1\n",
      "28-12-2020  396666         1\n",
      "29-01-2021  698583         1\n",
      "29-02-2020  0              1\n",
      "29-03-2020  5962           1\n",
      "29-03-2021  820716         1\n",
      "29-04-2020  24324          1\n",
      "29-05-2020  31946          1\n",
      "29-06-2020  41912          1\n",
      "29-07-2020  50613          1\n",
      "29-08-2020  57448          1\n",
      "29-09-2020  74717          1\n",
      "29-10-2020  132616         1\n",
      "29-11-2020  294799         1\n",
      "29-12-2020  400002         1\n",
      "30-01-2021  711018         1\n",
      "30-03-2020  6408           1\n",
      "30-03-2021  821105         1\n",
      "30-04-2020  24692          1\n",
      "30-05-2020  32203          1\n",
      "30-06-2020  42171          1\n",
      "30-07-2020  50868          1\n",
      "30-08-2020  57768          1\n",
      "30-09-2020  75542          1\n",
      "30-10-2020  137272         1\n",
      "30-11-2020  298061         1\n",
      "30-12-2020  406051         1\n",
      "31-01-2021  720516         1\n",
      "31-03-2020  7443           1\n",
      "31-03-2021  821722         1\n",
      "31-05-2020  32500          1\n",
      "31-07-2020  51072          1\n",
      "31-08-2020  58012          1\n",
      "31-10-2020  141279         1\n",
      "31-12-2020  413678         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_data_covid = prepare_data_covid(df_raw_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb899847",
   "metadata": {},
   "source": [
    "Em append_tem_dataframes, procedeu-se à junção de todos os datasets referentes aos dados atmosféricos de Portugal, para ser possível um tratamento mais fácil desses datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6da8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_temp_dataframes(df_raw_temp_2020_01, df_raw_temp_2020_02, df_raw_temp_2020_03, df_raw_temp_2020_04,\n",
    "                           df_raw_temp_2020_05, df_raw_temp_2020_06):\n",
    "    df_raw = df_raw_temp_2020_01.append(df_raw_temp_2020_02, ignore_index = True)\n",
    "    df_raw = df_raw.append(df_raw_temp_2020_03, ignore_index = True)\n",
    "    df_raw = df_raw.append(df_raw_temp_2020_04, ignore_index = True)\n",
    "    df_raw = df_raw.append(df_raw_temp_2020_05, ignore_index = True)\n",
    "    df_raw = df_raw.append(df_raw_temp_2020_06, ignore_index = True)\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    #print(df_raw)\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e9b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_appended_temp = append_temp_dataframes(df_raw_temp_2020_01,df_raw_temp_2020_02,df_raw_temp_2020_03,df_raw_temp_2020_04,df_raw_temp_2020_05,df_raw_temp_2020_06)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a1562b",
   "metadata": {},
   "source": [
    "Na função prepare_data_temp, encontra-se todo o processamento aplicado aos dados atmosféricos. Aqui, são eliminadas as coluna com missing values superiores a 80% da totalidade dos registos observados, e é executadado one-hot-encoding ao atributo Conditions através de aplicação da função apply()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0803905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_ohe_rain(condition):\n",
    "    res =0\n",
    "    if(condition == 'Rain' or condition == 'Rain, Partially cloudy'):\n",
    "        res=1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51cb89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_ohe_clear(condition):\n",
    "    res =0\n",
    "    if(condition == 'Clear'):\n",
    "        res=1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6023161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_ohe_cloudy(condition):\n",
    "    res =0\n",
    "    if(condition == 'Rain, Partially cloudy' or condition == 'Partially cloudy'):\n",
    "        res=1\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45bc9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_temp(df_raw):\n",
    "\n",
    "    df_aux = df_raw.copy()\n",
    "    #Ver se tem missing values\n",
    "    mv = df_aux.isnull().sum()\n",
    "    print(mv)\n",
    "\n",
    "\n",
    "    #drop de colunas com muitos missing values e a coluna nome do país, colunas referentes á neve(têm apenas valores ='s a 0)\n",
    "    df_aux = df_aux.drop(columns=['Name','Wind Chill','Heat Index', 'Wind Gust', 'Snow', 'Snow Depth'])\n",
    "\n",
    "    #rename columns\n",
    "    df_aux.columns = ['Date', 'Max_Temp','Min_Temp','Temperature','Precipitation','Wind_Speed','Wind_Direction','Visibility','Cloud_Cover','Relative_Humidity','Conditions']\n",
    "\n",
    "    #one hot encoding weather conditions\n",
    "    df_aux['Rain'] = np.nan\n",
    "    df_aux['Clear'] = np.nan\n",
    "    df_aux['Partially_cloudy'] = np.nan\n",
    "    \n",
    "    df_aux['Rain'] = df_aux.apply(lambda x: condition_ohe_rain(x['Conditions']), axis=1)\n",
    "    df_aux['Clear'] = df_aux.apply(lambda x: condition_ohe_clear(x['Conditions']), axis=1)\n",
    "    df_aux['Partially_cloudy'] = df_aux.apply(lambda x: condition_ohe_cloudy(x['Conditions']), axis=1)\n",
    "\n",
    "    df_aux = df_aux.drop(columns='Conditions')\n",
    "    #colocar data igual ao dataset daily_covid_19_portugal\n",
    "    df_aux[\"Date\"] = pd.to_datetime(df_aux[\"Date\"]).dt.strftime('%d-%m-%Y')\n",
    "\n",
    "    return df_aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dfc00d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                     0\n",
      "Date time                0\n",
      "Maximum Temperature      0\n",
      "Minimum Temperature      0\n",
      "Temperature              0\n",
      "Wind Chill             400\n",
      "Heat Index             399\n",
      "Precipitation            0\n",
      "Snow                     0\n",
      "Snow Depth               0\n",
      "Wind Speed               0\n",
      "Wind Direction           0\n",
      "Wind Gust              343\n",
      "Visibility               0\n",
      "Cloud Cover              0\n",
      "Relative Humidity        0\n",
      "Conditions               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_data_temp = prepare_data_temp(df_raw_appended_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676c4eb",
   "metadata": {},
   "source": [
    "Após o processamentodos diferentes datasets, procedeu-se à concatenação dos mesmos através da operação merge(df_data_covid,df_data_temp,on='Date',how='left'), uma vez os registos atmosféricos são de um periodo temporal superior aos registos temporais da pandemia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e5815b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifie_covid_datasets(df_data_covid, df_data_temp):\n",
    "\n",
    "    df_covid = pd.merge(df_data_covid,df_data_temp, on='Date', how='left')\n",
    "\n",
    "    print(df_data_covid.shape)\n",
    "    print(df_data_temp.shape)\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    return df_covid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d7f6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422, 40)\n",
      "(486, 13)\n"
     ]
    }
   ],
   "source": [
    "df_covid = unifie_covid_datasets(df_data_covid, df_data_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aecf88",
   "metadata": {},
   "source": [
    "Por fim é necessário criar o dataset final, que será utilizaos nas previsões sobre as mortes por covid em Portugal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71979ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confirmados</th>\n",
       "      <th>confirmados_arsnorte</th>\n",
       "      <th>confirmados_arscentro</th>\n",
       "      <th>confirmados_arslvt</th>\n",
       "      <th>confirmados_arsalentejo</th>\n",
       "      <th>confirmados_arsalgarve</th>\n",
       "      <th>confirmados_acores</th>\n",
       "      <th>confirmados_madeira</th>\n",
       "      <th>confirmados_novos</th>\n",
       "      <th>recuperados</th>\n",
       "      <th>obitos</th>\n",
       "      <th>internados_uci</th>\n",
       "      <th>obitos_arsnorte</th>\n",
       "      <th>obitos_arscentro</th>\n",
       "      <th>obitos_arslvt</th>\n",
       "      <th>obitos_arsalentejo</th>\n",
       "      <th>obitos_arsalgarve</th>\n",
       "      <th>obitos_acores</th>\n",
       "      <th>obitos_madeira</th>\n",
       "      <th>ativos</th>\n",
       "      <th>internados_enfermaria</th>\n",
       "      <th>confirmados_0_9</th>\n",
       "      <th>confirmados_10_19</th>\n",
       "      <th>confirmados_20_29</th>\n",
       "      <th>confirmados_30_39</th>\n",
       "      <th>confirmados_40_49</th>\n",
       "      <th>confirmados_50_59</th>\n",
       "      <th>confirmados_60_69</th>\n",
       "      <th>confirmados_70_79</th>\n",
       "      <th>confirmados_80_plus</th>\n",
       "      <th>obitos_0_9</th>\n",
       "      <th>obitos_10_19</th>\n",
       "      <th>obitos_20_29</th>\n",
       "      <th>obitos_30_39</th>\n",
       "      <th>obitos_40_49</th>\n",
       "      <th>obitos_50_59</th>\n",
       "      <th>obitos_60_69</th>\n",
       "      <th>obitos_70_79</th>\n",
       "      <th>obitos_80_plus</th>\n",
       "      <th>Max_Temp</th>\n",
       "      <th>Min_Temp</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Cloud_Cover</th>\n",
       "      <th>Relative_Humidity</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Clear</th>\n",
       "      <th>Partially_cloudy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>22.3</td>\n",
       "      <td>325.25</td>\n",
       "      <td>12.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>68.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>10.4</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.9</td>\n",
       "      <td>321.58</td>\n",
       "      <td>11.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>72.08</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>10.3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.7</td>\n",
       "      <td>123.33</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>79.20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>15.1</td>\n",
       "      <td>3.53</td>\n",
       "      <td>24.7</td>\n",
       "      <td>253.13</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.8</td>\n",
       "      <td>82.29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.68</td>\n",
       "      <td>38.6</td>\n",
       "      <td>235.63</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>92.37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            confirmados  confirmados_arsnorte  confirmados_arscentro  \\\n",
       "Date                                                                   \n",
       "2020-02-26          NaN                   NaN                    NaN   \n",
       "2020-02-27          0.0                   0.0                    0.0   \n",
       "2020-02-28          0.0                   0.0                    0.0   \n",
       "2020-02-29          0.0                   0.0                    0.0   \n",
       "2020-01-03          0.0                   0.0                    0.0   \n",
       "\n",
       "            confirmados_arslvt  confirmados_arsalentejo  \\\n",
       "Date                                                      \n",
       "2020-02-26                 NaN                      NaN   \n",
       "2020-02-27                 0.0                      0.0   \n",
       "2020-02-28                 0.0                      0.0   \n",
       "2020-02-29                 0.0                      0.0   \n",
       "2020-01-03                 0.0                      0.0   \n",
       "\n",
       "            confirmados_arsalgarve  confirmados_acores  confirmados_madeira  \\\n",
       "Date                                                                          \n",
       "2020-02-26                     NaN                 NaN                  NaN   \n",
       "2020-02-27                     0.0                 0.0                  0.0   \n",
       "2020-02-28                     0.0                 0.0                  0.0   \n",
       "2020-02-29                     0.0                 0.0                  0.0   \n",
       "2020-01-03                     0.0                 0.0                  0.0   \n",
       "\n",
       "            confirmados_novos  recuperados  obitos  internados_uci  \\\n",
       "Date                                                                 \n",
       "2020-02-26                  0          NaN     NaN             NaN   \n",
       "2020-02-27                  0          0.0     0.0             0.0   \n",
       "2020-02-28                  0          0.0     0.0             0.0   \n",
       "2020-02-29                  0          0.0     0.0             0.0   \n",
       "2020-01-03                  0          0.0     0.0             0.0   \n",
       "\n",
       "            obitos_arsnorte  obitos_arscentro  obitos_arslvt  \\\n",
       "Date                                                           \n",
       "2020-02-26              NaN               NaN            NaN   \n",
       "2020-02-27              0.0               0.0            0.0   \n",
       "2020-02-28              0.0               0.0            0.0   \n",
       "2020-02-29              0.0               0.0            0.0   \n",
       "2020-01-03              0.0               0.0            0.0   \n",
       "\n",
       "            obitos_arsalentejo  obitos_arsalgarve  obitos_acores  \\\n",
       "Date                                                               \n",
       "2020-02-26                 NaN                NaN            NaN   \n",
       "2020-02-27                 0.0                0.0            0.0   \n",
       "2020-02-28                 0.0                0.0            0.0   \n",
       "2020-02-29                 0.0                0.0            0.0   \n",
       "2020-01-03                 0.0                0.0            0.0   \n",
       "\n",
       "            obitos_madeira  ativos  internados_enfermaria  confirmados_0_9  \\\n",
       "Date                                                                         \n",
       "2020-02-26             NaN     NaN                    NaN              NaN   \n",
       "2020-02-27             0.0     0.0                    0.0              0.0   \n",
       "2020-02-28             0.0     0.0                    0.0              0.0   \n",
       "2020-02-29             0.0     0.0                    0.0              0.0   \n",
       "2020-01-03             0.0     0.0                    0.0              0.0   \n",
       "\n",
       "            confirmados_10_19  confirmados_20_29  confirmados_30_39  \\\n",
       "Date                                                                  \n",
       "2020-02-26                NaN                NaN                NaN   \n",
       "2020-02-27                0.0                0.0                0.0   \n",
       "2020-02-28                0.0                0.0                0.0   \n",
       "2020-02-29                0.0                0.0                0.0   \n",
       "2020-01-03                0.0                0.0                0.0   \n",
       "\n",
       "            confirmados_40_49  confirmados_50_59  confirmados_60_69  \\\n",
       "Date                                                                  \n",
       "2020-02-26                NaN                NaN                NaN   \n",
       "2020-02-27                0.0                0.0                0.0   \n",
       "2020-02-28                0.0                0.0                0.0   \n",
       "2020-02-29                0.0                0.0                0.0   \n",
       "2020-01-03                0.0                0.0                0.0   \n",
       "\n",
       "            confirmados_70_79  confirmados_80_plus  obitos_0_9  obitos_10_19  \\\n",
       "Date                                                                           \n",
       "2020-02-26                NaN                  NaN         NaN           NaN   \n",
       "2020-02-27                0.0                  0.0         0.0           0.0   \n",
       "2020-02-28                0.0                  0.0         0.0           0.0   \n",
       "2020-02-29                0.0                  0.0         0.0           0.0   \n",
       "2020-01-03                0.0                  0.0         0.0           0.0   \n",
       "\n",
       "            obitos_20_29  obitos_30_39  obitos_40_49  obitos_50_59  \\\n",
       "Date                                                                 \n",
       "2020-02-26           NaN           NaN           NaN           NaN   \n",
       "2020-02-27           0.0           0.0           0.0           0.0   \n",
       "2020-02-28           0.0           0.0           0.0           0.0   \n",
       "2020-02-29           0.0           0.0           0.0           0.0   \n",
       "2020-01-03           0.0           0.0           0.0           0.0   \n",
       "\n",
       "            obitos_60_69  obitos_70_79  obitos_80_plus  Max_Temp  Min_Temp  \\\n",
       "Date                                                                         \n",
       "2020-02-26           NaN           NaN             NaN      16.4      10.0   \n",
       "2020-02-27           0.0           0.0             0.0      20.4      10.4   \n",
       "2020-02-28           0.0           0.0             0.0      19.1      10.3   \n",
       "2020-02-29           0.0           0.0             0.0      18.0      13.1   \n",
       "2020-01-03           0.0           0.0             0.0      16.1      14.0   \n",
       "\n",
       "            Temperature  Precipitation  Wind_Speed  Wind_Direction  \\\n",
       "Date                                                                 \n",
       "2020-02-26         12.9           0.10        22.3          325.25   \n",
       "2020-02-27         14.5           0.00        16.9          321.58   \n",
       "2020-02-28         13.3           0.00        17.7          123.33   \n",
       "2020-02-29         15.1           3.53        24.7          253.13   \n",
       "2020-01-03         15.3           0.68        38.6          235.63   \n",
       "\n",
       "            Visibility  Cloud_Cover  Relative_Humidity  Rain  Clear  \\\n",
       "Date                                                                  \n",
       "2020-02-26        12.3          7.5              68.15     1      0   \n",
       "2020-02-27        11.8          3.9              72.08     0      1   \n",
       "2020-02-28        10.1          3.8              79.20     0      1   \n",
       "2020-02-29         9.6          6.8              82.29     1      0   \n",
       "2020-01-03         9.5          4.3              92.37     1      0   \n",
       "\n",
       "            Partially_cloudy  \n",
       "Date                          \n",
       "2020-02-26                 0  \n",
       "2020-02-27                 0  \n",
       "2020-02-28                 0  \n",
       "2020-02-29                 0  \n",
       "2020-01-03                 0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid[\"Date\"] = pd.to_datetime(df_covid[\"Date\"])\n",
    "df_covid = df_covid.set_index('Date')\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecc10eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv_file(df_covid, \"daily_covid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a513fc3",
   "metadata": {},
   "source": [
    "## Parte 2 - Daily_diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686595f",
   "metadata": {},
   "source": [
    "Aqui, são pormenorizas todas as escolhas e passos tomados, para que fosse possível construir um dataset final, capaz de ser robusto o suficiente para eventualmente se obter uma boa previsão para o número de mortes causadas por diabetes em Inglaterra. Para a construção deste dataset foram utilizados dados sobre a temperatura em Inglaterra através do [European Climate Assessment & Dataset](https://www.ecad.eu/dailydata/customquery.php), o registo diário do nível medio de Ozono através [UK Air - Department for environment Food & Rural Affairs](https://uk-air.defra.gov.uk/interactive-map), e o registo de mortes por diabetes no ano de 2014 através de [England Office for National Statistics](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/causesofdeath/adhocs/005259dailydeathoccurrencesbyallcausesanddiabetesmellitusicd10e10toe14englandandwales2012to2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a739ef",
   "metadata": {},
   "source": [
    "### Load do dataset Diabetes_2014_England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7f2dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_diabetes = load_datasets(\"daily_datasets/Diabetes_2014_England.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24fb79",
   "metadata": {},
   "source": [
    "### Load dos datasets das temperaturas em Inglaterra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61993068",
   "metadata": {},
   "source": [
    "Para podermos utilizar temperaturas médias registadas em Inglaterra, foi necessário recorrer a várias estações meteorológicas espalhas poir Inglaterra, as estações escolhidas foram as estações de Blackpool, Durham, Nottingham e Bognor Regis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "610eae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_temp_bognor_regis = load_normal_dataset(\"daily_datasets/mean_temp_bognor_regis.csv\")\n",
    "df_raw_temp_blackpool = load_normal_dataset(\"daily_datasets/mean_temp_blackpool.csv\")\n",
    "df_raw_temp_durham = load_normal_dataset(\"daily_datasets/mean_temp_durham.csv\")\n",
    "df_raw_temp_nottingham = load_normal_dataset(\"daily_datasets/mean_temp_nottingham.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a72a1",
   "metadata": {},
   "source": [
    "### Load dos datasets dos níveis de Ozono em Ingalterra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932ea86",
   "metadata": {},
   "source": [
    "Para podermos utilizar os níveis médios de ozono registadas em Inglaterra, foi necessário recorrer a várias estações meteorológicas espalhas poir Inglaterra, as estações escolhidas foram as estações de Hull (Freetown), Liverpool (Wirral Tranmere), London (Bloomsbury) e Norwich (Lakenfields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e347643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_ozone_hull = load_normal_dataset(\"daily_datasets/hull_freetown_ozone.csv\")\n",
    "df_raw_ozone_norwich = load_normal_dataset(\"daily_datasets/norwich_lakenfields_ozone.csv\")\n",
    "df_raw_ozone_wirral = load_normal_dataset(\"daily_datasets/wirral_tranmere_ozone.csv\")\n",
    "df_raw_ozone_london = load_normal_dataset(\"daily_datasets/london_ozone.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b8431",
   "metadata": {},
   "source": [
    "Após estes loads, foi necessário efetuar tratamento de dados obtidos, nomeadamente tratamento de missing values, alterações dos tipos dos atributos, cálculo de média de medidas de ozono e temperatura e por fim a concatenação dos diferentes datasets num só dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f60c42",
   "metadata": {},
   "source": [
    "### Tratamento dos datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e391e",
   "metadata": {},
   "source": [
    "Em prepare_data_diabetes(), processamos os dados relativos ao número de mortes por diabetes em Inglaterra em 2014. O processamento foi mínimo, apenas os atributos foram renomeados e os seus tipos foram alterado como sendo numéricos numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a0c725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_diabetes(df_raw):\n",
    "\n",
    "    df_aux = df_raw.copy()\n",
    "\n",
    "    df_aux.columns = ['Date','All_Causes','Diabetes']\n",
    "    #print(df_aux)\n",
    "    \n",
    "    #tirar o espaço da string All_Causes\n",
    "    df_aux['All_Causes'] = df_aux['All_Causes'].str.replace(\" \",'')\n",
    "\n",
    "    # alterar tipo das colunas All_Causes, Diabetes para numerico :\n",
    "    # Alterar tipo da coluna Date para datetime\n",
    "    df_aux[\"Diabetes\"] = pd.to_numeric(df_aux[\"Diabetes\"])\n",
    "    df_aux['All_Causes'] = pd.to_numeric(df_aux['All_Causes'])\n",
    "\n",
    "    #Ver se tem duplicados\n",
    "    r  = df_aux.duplicated(subset='Date').sum()\n",
    "    print(r)\n",
    "\n",
    "    #Ver se tem missing values\n",
    "    mv = df_aux.isnull().sum()\n",
    "    print(mv)\n",
    "\n",
    "    #somar mortes no mesmo dia\n",
    "    #df_aux = df_aux.groupby(['Date']).sum()\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    #print(df_aux)\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e6df496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Date          0\n",
      "All_Causes    0\n",
      "Diabetes      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "f_data_diabetes = prepare_data_diabetes(df_raw_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5e4a7",
   "metadata": {},
   "source": [
    "Para o tratamentos dos datasets que contêm as temperaturas diárias registadas em Inglaterra, para cada um destes datasets foi necessário transformar a temperatura registada num valor real, atravéss da sua multiplicação por 0.1. Também foi necessário transformar a data do registo num formato válido, para depois ser possível concatenar os vários datasets pela sua data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed98ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_temperature_england(df_raw,name):\n",
    "\n",
    "    df_aux = df_raw.copy()\n",
    "    print(df_aux.dtypes)\n",
    "    print (df_aux.head())\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    #print(df_aux)\n",
    "    #unique values of Station id (só tem 1 estaçao)\n",
    "    print (len(df_aux['STAID'].unique()))\n",
    "    #unique values od date (não tem duplicados)\n",
    "    print(len(df_aux['DATE'].unique()))\n",
    "\n",
    "    #drop colunas STAID,Q_TG\n",
    "    df_aux = df_aux.drop(columns=['SOUID','STAID','Q_TG'])\n",
    "\n",
    "    #tornar valores da temperatura, em valores reais   temperatura*0.1\n",
    "    df_aux['TG'] = df_aux['TG']*0.1\n",
    "\n",
    "    #converter integer date para a data correta: 19600101 para 1960-01-01\n",
    "    df_aux['DATE'] = pd.to_datetime(df_aux['DATE'], format='%Y%m%d')\n",
    "\n",
    "    #modificar data de 1960-01-01 para 01/01/1960\n",
    "    df_aux[\"DATE\"] = pd.to_datetime(df_aux[\"DATE\"]).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "    nome ='temp_'+name\n",
    "    #renomear colunas\n",
    "    df_aux.columns = ['Date', nome]\n",
    "    #print(df_aux.head())\n",
    "    #print(df_aux.dtypes)\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36ba763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAID    int64\n",
      "SOUID    int64\n",
      "DATE     int64\n",
      "TG       int64\n",
      "Q_TG     int64\n",
      "dtype: object\n",
      "   STAID   SOUID      DATE   TG  Q_TG\n",
      "0   1863  107057  19600101  106     0\n",
      "1   1863  107057  19600102   86     0\n",
      "2   1863  107057  19600103   89     0\n",
      "3   1863  107057  19600104   97     0\n",
      "4   1863  107057  19600105  102     0\n",
      "1\n",
      "22371\n",
      "STAID    int64\n",
      "SOUID    int64\n",
      "DATE     int64\n",
      "TG       int64\n",
      "Q_TG     int64\n",
      "dtype: object\n",
      "   STAID   SOUID      DATE  TG  Q_TG\n",
      "0  11102  156831  20130101  66     0\n",
      "1  11102  156831  20130102  79     0\n",
      "2  11102  156831  20130103  78     0\n",
      "3  11102  156831  20130104  86     0\n",
      "4  11102  156831  20130105  71     0\n",
      "1\n",
      "3012\n",
      "STAID    int64\n",
      "SOUID    int64\n",
      "DATE     int64\n",
      "TG       int64\n",
      "Q_TG     int64\n",
      "dtype: object\n",
      "   STAID   SOUID      DATE    TG  Q_TG\n",
      "0    461  156851  18800101 -9999     9\n",
      "1    461  156851  18800102    80     0\n",
      "2    461  156851  18800103    85     1\n",
      "3    461  156851  18800104    66     0\n",
      "4    461  156851  18800105    50     0\n",
      "1\n",
      "51590\n",
      "STAID    int64\n",
      "SOUID    int64\n",
      "DATE     int64\n",
      "TG       int64\n",
      "Q_TG     int64\n",
      "dtype: object\n",
      "   STAID   SOUID      DATE  TG  Q_TG\n",
      "0   1850  107044  19600101  92     0\n",
      "1   1850  107044  19600102  50     0\n",
      "2   1850  107044  19600103  69     0\n",
      "3   1850  107044  19600104  78     0\n",
      "4   1850  107044  19600105  70     0\n",
      "1\n",
      "22371\n"
     ]
    }
   ],
   "source": [
    "f_data_temp_bognor_regis = prepare_data_temperature_england(df_raw_temp_bognor_regis, 'BOGNOR')\n",
    "f_data_temp_blackpool = prepare_data_temperature_england(df_raw_temp_blackpool, 'BLACKP')\n",
    "f_data_temp_durham = prepare_data_temperature_england(df_raw_temp_durham, 'DURHAM')\n",
    "f_data_temp_nottingham = prepare_data_temperature_england(df_raw_temp_nottingham, 'NOTTI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923cbda",
   "metadata": {},
   "source": [
    "Após os tratementos dos diferentes datasets da temperatura em Inglaterra, e para utilizar-mos uma temperatura que representase Inglaterra como um todos foi necessário calcular a temperatura média para Inglaterra. Essa temperatura foi obtida através do método unifie_temp_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "522e90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifie_temp_mean(f_data_temp_bognor_regis,f_data_temp_blackpool,f_data_temp_durham,f_data_temp_nottingham):\n",
    "    df_diabetes = pd.merge(f_data_temp_bognor_regis,f_data_temp_blackpool, on='Date', how='inner')\n",
    "    df_diabetes = pd.merge(df_diabetes, f_data_temp_durham, on='Date', how='inner')\n",
    "    df_diabetes = pd.merge(df_diabetes, f_data_temp_nottingham, on='Date', how='inner')\n",
    "    #calculo da temperatura media\n",
    "    df_diabetes['Temperature'] = df_diabetes.iloc[:, 1:4].mean(axis=1)\n",
    "    # drop das colunas de temperatura das cidades\n",
    "    cols= [1,2,3,4]\n",
    "    df_diabetes = df_diabetes.drop(df_diabetes.columns[cols], axis=1)\n",
    "    #print(df_diabetes.shape)\n",
    "    #print(df_diabetes.head())\n",
    "    return df_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c5fb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_england = unifie_temp_mean(f_data_temp_bognor_regis, f_data_temp_blackpool, f_data_temp_durham, f_data_temp_nottingham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e9ce7",
   "metadata": {},
   "source": [
    "Foi também necessário efetuar tratamento dos dados constantes nos datasets provenientes de diferentes centros de observação do nível de Ozono em Inglaterra. No método prepare_data_ozone_england(), uma vez que os registos eram diários e eram efectuadas medições do Ozono de hora a hora, os missing values foram substituidos pela média diária, e a data diária do registo foi colocada para um formato que fosse depois possível juntar com os outros datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b296e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_ozone_england(df_raw,name):\n",
    "    df_aux = df_raw.copy()\n",
    "\n",
    "    #Ver se tem missing values\n",
    "    mv = df_aux.isnull().sum()\n",
    "    print('Estacao: ',name,' nº: ',mv)\n",
    "    #print(df_aux.columns)\n",
    "    #substituir missing values pelo valor médio da hora\n",
    "    mean1,mean2,mean3,mean4,mean5,mean6,mean7,mean8,mean9,mean10,mean11,mean12 = df_aux['01:00'].mean(),df_aux['02:00'].mean(),df_aux['03:00'].mean(),df_aux['04:00'].mean(),df_aux['05:00'].mean(), df_aux['06:00'].mean(),df_aux['07:00'].mean(),df_aux['08:00'].mean(),df_aux['09:00'].mean(),df_aux['10:00'].mean(),df_aux['11:00'].mean(),df_aux['12:00'].mean()\n",
    "    mean13, mean14, mean15, mean16, mean17, mean18, mean19, mean20, mean21, mean22, mean23, mean24 =  df_aux['13:00'].mean(),df_aux['14:00'].mean(),df_aux['15:00'].mean(),df_aux['16:00'].mean(),df_aux['17:00'].mean(),df_aux['18:00'].mean(),df_aux['19:00'].mean(),df_aux['20:00'].mean(),df_aux['21:00'].mean(),df_aux['22:00'].mean(),df_aux['23:00'].mean(),df_aux['24:00'].mean()\n",
    "\n",
    "\n",
    "    df_aux['01:00'] = df_aux['01:00'].fillna(mean1)\n",
    "    df_aux['02:00'] = df_aux['02:00'].fillna(mean2)\n",
    "    df_aux['03:00'] = df_aux['03:00'].fillna(mean3)\n",
    "    df_aux['04:00'] = df_aux['04:00'].fillna(mean4)\n",
    "    df_aux['05:00'] = df_aux['05:00'].fillna(mean5)\n",
    "    df_aux['06:00'] = df_aux['06:00'].fillna(mean6)\n",
    "    df_aux['07:00'] = df_aux['07:00'].fillna(mean7)\n",
    "    df_aux['08:00'] = df_aux['08:00'].fillna(mean8)\n",
    "    df_aux['09:00'] = df_aux['09:00'].fillna(mean9)\n",
    "    df_aux['10:00'] = df_aux['10:00'].fillna(mean10)\n",
    "    df_aux['11:00'] = df_aux['11:00'].fillna(mean11)\n",
    "    df_aux['12:00'] = df_aux['12:00'].fillna(mean12)\n",
    "    df_aux['13:00'] = df_aux['13:00'].fillna(mean13)\n",
    "    df_aux['14:00'] = df_aux['14:00'].fillna(mean14)\n",
    "    df_aux['15:00'] = df_aux['15:00'].fillna(mean15)\n",
    "    df_aux['16:00'] = df_aux['16:00'].fillna(mean16)\n",
    "    df_aux['17:00'] = df_aux['17:00'].fillna(mean17)\n",
    "    df_aux['18:00'] = df_aux['18:00'].fillna(mean18)\n",
    "    df_aux['19:00'] = df_aux['19:00'].fillna(mean19)\n",
    "    df_aux['20:00'] = df_aux['20:00'].fillna(mean20)\n",
    "    df_aux['21:00'] = df_aux['21:00'].fillna(mean21)\n",
    "    df_aux['22:00'] = df_aux['22:00'].fillna(mean22)\n",
    "    df_aux['23:00'] = df_aux['23:00'].fillna(mean23)\n",
    "    df_aux['24:00'] = df_aux['24:00'].fillna(mean24)\n",
    "\n",
    "    # Ver se tem missing values\n",
    "    #mv = df_aux.isnull().sum()\n",
    "    #print('Estacao: ', name, ' nº: ', mv)\n",
    "\n",
    "    #coluna media diária de ozono\n",
    "    nome = 'ozone'+ name\n",
    "    df_aux[nome] = df_aux.iloc[:, 1:25].mean(axis=1)\n",
    "\n",
    "    #drop colunas das horas\n",
    "    cols = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "    df_aux = df_aux.drop(df_aux.columns[cols], axis=1)\n",
    "\n",
    "    #modificar data de 1960-01-01 para 01/01/1960\n",
    "    df_aux[\"Date\"] = pd.to_datetime(df_aux[\"Date\"]).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa19f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estacao:  HULL  nº:  Date      0\n",
      "01:00    16\n",
      "02:00    13\n",
      "03:00    12\n",
      "04:00    12\n",
      "05:00    12\n",
      "06:00    12\n",
      "07:00    12\n",
      "08:00    13\n",
      "09:00    17\n",
      "10:00    17\n",
      "11:00    14\n",
      "12:00    13\n",
      "13:00    15\n",
      "14:00    15\n",
      "15:00    13\n",
      "16:00    12\n",
      "17:00    12\n",
      "18:00    12\n",
      "19:00    12\n",
      "20:00    12\n",
      "21:00    12\n",
      "22:00    13\n",
      "23:00    12\n",
      "24:00    13\n",
      "dtype: int64\n",
      "Estacao:  NORWICH  nº:  Date     0\n",
      "01:00    2\n",
      "02:00    2\n",
      "03:00    2\n",
      "04:00    2\n",
      "05:00    2\n",
      "06:00    2\n",
      "07:00    2\n",
      "08:00    2\n",
      "09:00    4\n",
      "10:00    4\n",
      "11:00    3\n",
      "12:00    6\n",
      "13:00    6\n",
      "14:00    4\n",
      "15:00    3\n",
      "16:00    2\n",
      "17:00    3\n",
      "18:00    2\n",
      "19:00    2\n",
      "20:00    2\n",
      "21:00    2\n",
      "22:00    2\n",
      "23:00    2\n",
      "24:00    2\n",
      "dtype: int64\n",
      "Estacao:  WINRRAL  nº:  Date     0\n",
      "01:00    3\n",
      "02:00    2\n",
      "03:00    2\n",
      "04:00    2\n",
      "05:00    2\n",
      "06:00    2\n",
      "07:00    2\n",
      "08:00    2\n",
      "09:00    2\n",
      "10:00    3\n",
      "11:00    4\n",
      "12:00    7\n",
      "13:00    7\n",
      "14:00    7\n",
      "15:00    7\n",
      "16:00    6\n",
      "17:00    3\n",
      "18:00    3\n",
      "19:00    3\n",
      "20:00    3\n",
      "21:00    3\n",
      "22:00    3\n",
      "23:00    3\n",
      "24:00    3\n",
      "dtype: int64\n",
      "Estacao:  LONDON  nº:  Date      0\n",
      "01:00     6\n",
      "02:00     2\n",
      "03:00     2\n",
      "04:00     2\n",
      "05:00     2\n",
      "06:00     2\n",
      "07:00     2\n",
      "08:00     2\n",
      "09:00     2\n",
      "10:00     2\n",
      "11:00     5\n",
      "12:00    10\n",
      "13:00    10\n",
      "14:00     6\n",
      "15:00     5\n",
      "16:00     3\n",
      "17:00     2\n",
      "18:00     2\n",
      "19:00     2\n",
      "20:00     2\n",
      "21:00     2\n",
      "22:00     2\n",
      "23:00     2\n",
      "24:00     2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "f_data_ozone_hull = prepare_data_ozone_england(df_raw_ozone_hull, 'HULL')\n",
    "f_data_ozone_norwich = prepare_data_ozone_england(df_raw_ozone_norwich, 'NORWICH')\n",
    "f_data_ozone_wirral = prepare_data_ozone_england(df_raw_ozone_wirral, 'WINRRAL')\n",
    "f_data_ozone_london = prepare_data_ozone_england(df_raw_ozone_london, 'LONDON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21394987",
   "metadata": {},
   "source": [
    "Após o tratamento dos diferentes datasets do Ozono, procedeu-se á sua concatenação, formando apenas um dataframe com o registo diário de medição do nível médio de Ozono em Inglaterra , obtido através do cálculo da media dos valores registados nas diferentes estações de medição. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36178f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifie_ozone_mean(f_data_ozone_hull,f_data_ozone_norwich,f_data_ozone_wirral,f_data_ozone_london):\n",
    "    df_ozone = pd.merge(f_data_ozone_hull,f_data_ozone_norwich, on='Date', how='inner')\n",
    "    df_ozone = pd.merge(df_ozone,f_data_ozone_wirral, on='Date', how='inner')\n",
    "    df_ozone = pd.merge(df_ozone, f_data_ozone_london, on='Date', how='inner')\n",
    "    #print(df_ozone.shape)\n",
    "    #drop das colunas do ozone das cidades\n",
    "    df_ozone['Ozone'] = df_ozone.iloc[:, 1:4].mean(axis=1)\n",
    "    cols= [1,2,3,4]\n",
    "    df_ozone = df_ozone.drop(df_ozone.columns[cols], axis=1)\n",
    "    print(df_ozone.shape)\n",
    "    print(df_ozone.head())\n",
    "    return df_ozone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b09bd66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 2)\n",
      "         Date      Ozone\n",
      "0  01/01/2014  54.285472\n",
      "1  01/02/2014  48.550482\n",
      "2  01/03/2014  60.489870\n",
      "3  01/04/2014  49.537995\n",
      "4  01/05/2014  51.024612\n"
     ]
    }
   ],
   "source": [
    "df_ozone_england = unifie_ozone_mean(f_data_ozone_hull, f_data_ozone_norwich, f_data_ozone_wirral, f_data_ozone_london)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab69e0",
   "metadata": {},
   "source": [
    "Após o tratamento dos datasets de Diabetes, Temperatura e Ozono, procedeu-se então à criação do dataset final, que será utilizado nas previsões de Séries Temporais, para isso utilizou-se o método unifie_diabetes_datasets()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1f4bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifie_diabetes_datasets(f_data_diabetes, f_data_temp, f_data_ozone):\n",
    "\n",
    "    df_diabetes = pd.merge(f_data_diabetes,f_data_temp, on='Date', how='inner')\n",
    "    df_diabetes = pd.merge(df_diabetes, f_data_ozone, on='Date', how='inner')\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    print(df_diabetes.head())\n",
    "\n",
    "    return df_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "638a9f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  All_Causes  Diabetes      Ozone  Temperature\n",
      "0  01/01/2014        1418        15  54.285472     6.966667\n",
      "1  02/01/2014        1485        17  62.274416     7.200000\n",
      "2  03/01/2014        1456        25  34.028751     7.600000\n",
      "3  04/01/2014        1448        13  35.512845     6.366667\n",
      "4  05/01/2014        1411        20  62.606071     5.200000\n"
     ]
    }
   ],
   "source": [
    "df_diabetes = unifie_diabetes_datasets(f_data_diabetes, df_ozone_england, df_temp_england)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3b262",
   "metadata": {},
   "source": [
    "Ainda antes de passar para ficheiro CSV, a data dos registos foi tornada como row.index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fda668c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            All_Causes  Diabetes      Ozone  Temperature\n",
      "Date                                                    \n",
      "2014-01-01        1418        15  54.285472     6.966667\n",
      "2014-02-01        1485        17  62.274416     7.200000\n",
      "2014-03-01        1456        25  34.028751     7.600000\n",
      "2014-04-01        1448        13  35.512845     6.366667\n",
      "2014-05-01        1411        20  62.606071     5.200000\n"
     ]
    }
   ],
   "source": [
    "df_diabetes[\"Date\"] = pd.to_datetime(df_diabetes[\"Date\"])\n",
    "df_diabetes = df_diabetes.set_index('Date')\n",
    "print(df_diabetes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447970eb",
   "metadata": {},
   "source": [
    "Finalmente, através do método to_csv_file, foi criado o ficheiro CSV do datset final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "decb2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv_file(df_diabetes, \"daily_diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3557f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
