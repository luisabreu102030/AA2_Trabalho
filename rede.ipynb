{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suited-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "# to determine the most voted\n",
    "import collections \n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "overhead-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == classNames\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [32,32])\n",
    "\n",
    "def get_bytes_and_label(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "    columns = 6\n",
    "    rows = BATCH_SIZE / columns + 1  \n",
    "    plt.figure(figsize=(10, 2 * rows))\n",
    "    for n in range(BATCH_SIZE):\n",
    "        ax = plt.subplot(int(rows), columns, n+1)\n",
    "        plt.imshow((image_batch[n]))\n",
    "        plt.title(classNames[label_batch[n]==1][0])\n",
    "        plt.axis('off')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cleared-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_callbacks(file_path):\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath= file_path, \n",
    "                               monitor = 'val_accuracy',\n",
    "                               verbose=1, \n",
    "                               save_weights_only=True,\n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "    earlyStopper = EarlyStopping(monitor='val_loss', min_delta = 0.0001, patience = 15, verbose = 1)\n",
    "\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.000000001, verbose = 1)\n",
    "\n",
    "    return [checkpointer, earlyStopper, reduceLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "processed-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 32\n",
    "NUM_MODELS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "instrumental-amplifier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00000', '00001', '00002', '00003', '00004', '00005', '00006',\n",
       "       '00007', '00008', '00009', '00010', '00011', '00012', '00013',\n",
       "       '00014', '00015', '00016', '00017', '00018', '00019', '00020',\n",
       "       '00021', '00022', '00023', '00024', '00025', '00026', '00027',\n",
       "       '00028', '00029', '00030', '00031', '00032', '00033', '00034',\n",
       "       '00035', '00036', '00037', '00038', '00039', '00040', '00041',\n",
       "       '00042'], dtype='<U5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = pathlib.Path('Final_Training/Images/')\n",
    "  \n",
    "classNames = np.array(os.listdir(data_dir))\n",
    "NUM_CLASSES = len(classNames)\n",
    "classNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "musical-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3) (43,)\n",
      "Total images in dataset:  27930\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "listset = tf.data.Dataset.list_files(\"Final_Training/Images/*/*.png\")\n",
    "dataset = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "t = next(iter(dataset))\n",
    "print(t[0].shape, t[1].shape)\n",
    "\n",
    "dataset_length = tf.data.experimental.cardinality(dataset).numpy()\n",
    "print(\"Total images in dataset: \", dataset_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "boxed-passenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in validatation dataset:  11280\n",
      "Total images in test dataset:  12630\n"
     ]
    }
   ],
   "source": [
    "val_listset = tf.data.Dataset.list_files(\"Final_Validation/Images/*/*.png\")\n",
    "val_dataset_length = val_listset.cardinality().numpy()\n",
    "print(\"Total images in validatation dataset: \", val_dataset_length)\n",
    "\n",
    "valset = val_listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "valset = valset.cache()\n",
    "valset = valset.shuffle(buffer_size = val_dataset_length)\n",
    "valset = valset.batch(batch_size = BATCH_SIZE)\n",
    "valset = valset.prefetch(buffer_size = AUTOTUNE)\n",
    "\n",
    "\n",
    "test_listset = tf.data.Dataset.list_files(\"Final_Test/Images/*/*.png\")\n",
    "test_dataset_length = test_listset.cardinality().numpy()\n",
    "print(\"Total images in test dataset: \", test_dataset_length)\n",
    "\n",
    "testset = test_listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "testset = testset.batch(batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incomplete-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(classCount, imgSize, channels):\n",
    "\n",
    "    modelLogits = Sequential()\n",
    "    \n",
    "    modelLogits.add(Conv2D(128, (5, 5),\n",
    "                input_shape=(imgSize, imgSize, channels)))         \n",
    "    modelLogits.add(LeakyReLU(alpha=0.01))  \n",
    "    modelLogits.add(BatchNormalization())\n",
    "    modelLogits.add(Dropout(0.5)) \n",
    "\n",
    "    modelLogits.add(Conv2D(196, (5, 5) )) \n",
    "    modelLogits.add(LeakyReLU(alpha=0.01))\n",
    "    modelLogits.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    modelLogits.add(BatchNormalization())\n",
    "    modelLogits.add(Dropout(0.5)) \n",
    "\n",
    "    modelLogits.add(Conv2D(256, (5, 5) ) )   \n",
    "    modelLogits.add(LeakyReLU(alpha=0.01))\n",
    "    modelLogits.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    modelLogits.add(BatchNormalization())\n",
    "    modelLogits.add(Dropout(0.5)) \n",
    "    \n",
    "    modelLogits.add(Flatten())\n",
    "    modelLogits.add(Dense(384))\n",
    "    modelLogits.add(LeakyReLU(alpha=0.0))             \n",
    "    modelLogits.add(Dropout(0.5)) \n",
    "    \n",
    "    modelLogits.add(Dense(classCount))\n",
    "    \n",
    "    output = Activation('softmax')(modelLogits.output)\n",
    "\n",
    "    model = tf.keras.Model(modelLogits.inputs, output)\n",
    "    \n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer = opt, loss='categorical_crossentropy', metrics=[ 'accuracy'])\n",
    "    return model, modelLogits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-checkout",
   "metadata": {},
   "source": [
    "### Data augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "changing-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "def process_brightness(image, label):\n",
    "    \n",
    "    img = tf.clip_by_value(tfa.image.random_hsv_in_yiq(image, 0.0, 1.0, 1.0, 0.1, 3.0),0,1)\n",
    "    return img, label\n",
    "\n",
    "def process_saturation(image, label):\n",
    "    \n",
    "    img = tf.clip_by_value(tfa.image.random_hsv_in_yiq(image, 0.0, 1.0, 3.0, 1.0, 1.0),0,1)\n",
    "    return img, label\n",
    "\n",
    "def process_contrast(image, label):\n",
    "    \n",
    "    img = tf.clip_by_value(tf.image.random_contrast(image, lower=0.1, upper=3.0, seed=None), 0, 1)\n",
    "    return img, label\n",
    "\n",
    "def process_hue(image, label):\n",
    "    \n",
    "    img = tf.image.random_hue(image, max_delta=0.2, seed=None)\n",
    "    return img, label\n",
    "\n",
    "def process_rotate(image, label):\n",
    "    \n",
    "    img = tfa.image.rotate(image, tf.random.uniform(shape=(), minval=-0.175, maxval=0.175))\n",
    "    return img, label\n",
    "\n",
    "def process_shear(image, label):\n",
    "    \n",
    "    img = tfa.image.rotate(image, tf.random.uniform(shape=(), minval=-0.175, maxval=0.175))\n",
    "    sx = tf.random.uniform(shape=(), minval=-0.1, maxval=0.1, dtype=tf.dtypes.float32)\n",
    "    img = tfa.image.transform(img, [1, sx, -sx*32,   0,1,0,  0,0])\n",
    "    return img, label\n",
    "\n",
    "def process_translate(image, label):\n",
    "\n",
    "    img = tfa.image.rotate(image, tf.random.uniform(shape=(), minval=-0.175, maxval=0.175))\n",
    "    tx = tf.random.uniform(shape=(), minval=-3, maxval=3, dtype=tf.dtypes.float32)\n",
    "    ty = tf.random.uniform(shape=(), minval=-3, maxval=3, dtype=tf.dtypes.float32)  \n",
    "    img = tfa.image.translate(img, [tx,ty])\n",
    "    return img, label\n",
    "\n",
    "def process_crop(image, label):\n",
    "    \n",
    "    c = tf.random.uniform(shape=(), minval=24, maxval=32, dtype=tf.dtypes.float32)\n",
    "    img = tf.image.random_crop(image, size=[c,c,3])\n",
    "    img = tf.image.resize(img ,size= [32,32])\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-fraud",
   "metadata": {},
   "source": [
    "### Ensemble functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adverse-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train, val,file_path_prefix, new_length):\n",
    "    models = []\n",
    "    histories = []\n",
    "    \n",
    "    for i in range(NUM_MODELS):\n",
    "\n",
    "        model, modelL = create_model(NUM_CLASSES,IMAGE_SIZE,3)\n",
    "\n",
    "        callbacks = prepare_callbacks(f'{file_path_prefix}_{i:02}/cp.ckpt')\n",
    "        \n",
    "        hist = model.fit(train, steps_per_epoch = new_length / BATCH_SIZE,\n",
    "                              epochs=100, \n",
    "                              validation_data = val, \n",
    "                              callbacks = callbacks)\n",
    "\n",
    "        models.append([model, modelL])\n",
    "        histories.append(hist)\n",
    "    \n",
    "    return models,histories\n",
    "\n",
    "\n",
    "def create_models():\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    for i in range(NUM_MODELS):\n",
    "        model, modelL = create_model(NUM_CLASSES,IMAGE_SIZE,3)\n",
    "        models.append([model, modelL])\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def load_weights(models, file_path_prefix):\n",
    "    for i in range(NUM_MODELS):\n",
    "        file_path = f'{file_path_prefix}_{i:02}/cp.ckpt'\n",
    "        models[i][0].load_weights(file_path)\n",
    "        models[i][0].save('ensemble/temp.hdf5')\n",
    "        models[i][1].load_weights('ensemble/temp.hdf5', by_name = True)\n",
    "\n",
    "\n",
    "def evaluate_models(models):\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    for i in range(NUM_MODELS):\n",
    "        eval = models[i][0].evaluate(testset, verbose = 2)\n",
    "        accuracy += eval[1]\n",
    "        \n",
    "    print(f'average accuracy: {(accuracy/NUM_MODELS)*100:.3f}')\n",
    "\n",
    "    \n",
    "def get_labels_logits_and_preds(models):\n",
    "\n",
    "    preds = [[] for _ in range(NUM_MODELS) ]\n",
    "    logits = [[] for _ in range(NUM_MODELS)]\n",
    "    labels = []\n",
    "    for images, labs in testset.take(-1):\n",
    "\n",
    "        labels.extend(labs.numpy())\n",
    "        for i in range(NUM_MODELS):\n",
    "\n",
    "            preds[i].extend(models[i][0].predict(images))\n",
    "            logits[i].extend(models[i][1].predict(images))\n",
    "\n",
    "    labels = [np.argmax(i) for i in labels]  \n",
    "    \n",
    "    return labels, logits, preds\n",
    "\n",
    "\n",
    "def get_class_preds(preds):\n",
    "\n",
    "    class_preds = []\n",
    "\n",
    "    for i in range(test_dataset_length):\n",
    "\n",
    "        c = []\n",
    "        for m in range(NUM_MODELS):\n",
    "\n",
    "            c.append(np.argmax(preds[m][i]))\n",
    "        class_preds.append(c)\n",
    "        \n",
    "    return class_preds\n",
    "\n",
    "\n",
    "def get_class_from_sum_of_logits(logits):\n",
    "\n",
    "    sum_logits = []\n",
    "\n",
    "    for i in range(test_dataset_length):\n",
    "\n",
    "        log = logits[0][i]\n",
    "        for m in range(1, NUM_MODELS):\n",
    "            log = np.add(log, logits[m][i])\n",
    "        sum_logits.append(np.argmax(log))\n",
    "    return(sum_logits)\n",
    "\n",
    "\n",
    "def get_stats(labels, class_preds, class_logits):\n",
    "\n",
    "    all_correct = 0\n",
    "    all_incorrect = 0\n",
    "    maj_vote = 0\n",
    "    maj_wrong = 0\n",
    "    tie = 0\n",
    "    count = 0\n",
    "    log_ok = 0\n",
    "    log_ko = 0\n",
    "\n",
    "    for k in range(test_dataset_length):\n",
    "\n",
    "        counter = collections.Counter(class_preds[k])\n",
    "        if len(counter) == 1:\n",
    "            if counter.most_common(1)[0][0] == labels[k]:\n",
    "                all_correct += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "        else:\n",
    "            aux = counter.most_common(2)\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] == labels[k]:\n",
    "                maj_vote += 1\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] != labels[k]:\n",
    "                maj_wrong += 1\n",
    "            elif aux[0][1] == aux[1][1]:\n",
    "                tie += 1\n",
    "        if class_logits[k] == labels[k]:\n",
    "            log_ok += 1\n",
    "        else:\n",
    "            log_ko += 1\n",
    "        count += 1 \n",
    "        \n",
    "    return [count, all_correct, all_incorrect, maj_vote, tie, maj_wrong, log_ok, log_ko]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-worst",
   "metadata": {},
   "source": [
    "### Dataset augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "municipal-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataV1 = dataset\n",
    "# color ops\n",
    "dataV1 = dataV1.map(process_brightness)\n",
    "dataV1 = dataV1.concatenate(dataset.map(process_contrast))\n",
    "dataV1 = dataV1.concatenate(dataset.map(process_hue))\n",
    "dataV1 = dataV1.concatenate(dataset.map(process_saturation))\n",
    "\n",
    "#geometry ops\n",
    "dataV1 = dataV1.concatenate(dataset.map(process_rotate))\n",
    "dataV1 = dataV1.concatenate(dataset.map(process_shear))\n",
    "dataV1 = dataV1.concatenate(dataset.map(process_translate))\n",
    "dataV1 = dataV1.concatenate(dataset.map(process_crop))\n",
    "\n",
    "# number of dataset augmentation functions used\n",
    "dataset_updated_length = dataset_length * 8\n",
    "\n",
    "dataV1 = dataV1.cache()\n",
    "dataV1 = dataV1.shuffle(buffer_size = (dataset_updated_length))\n",
    "dataV1 = dataV1.batch(batch_size = BATCH_SIZE)\n",
    "dataV1 = dataV1.prefetch(buffer_size = AUTOTUNE)\n",
    "dataV1 = dataV1.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "united-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1745/1745 [==============================] - 305s 109ms/step - loss: 1.3625 - accuracy: 0.6500 - val_loss: 0.1558 - val_accuracy: 0.9576\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.95762, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 2/100\n",
      "1745/1745 [==============================] - 185s 106ms/step - loss: 0.3663 - accuracy: 0.8962 - val_loss: 0.0724 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.95762 to 0.97713, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 3/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.1984 - accuracy: 0.9431 - val_loss: 0.0456 - val_accuracy: 0.9859\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.97713 to 0.98590, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 4/100\n",
      "1745/1745 [==============================] - 185s 106ms/step - loss: 0.1286 - accuracy: 0.9621 - val_loss: 0.0357 - val_accuracy: 0.9887\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.98590 to 0.98865, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 5/100\n",
      "1745/1745 [==============================] - 185s 106ms/step - loss: 0.0941 - accuracy: 0.9724 - val_loss: 0.0332 - val_accuracy: 0.9911\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.98865 to 0.99113, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 6/100\n",
      "1745/1745 [==============================] - 185s 106ms/step - loss: 0.0721 - accuracy: 0.9782 - val_loss: 0.0232 - val_accuracy: 0.99392 - ac\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.99113 to 0.99388, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 7/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0593 - accuracy: 0.9819 - val_loss: 0.0219 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.99388\n",
      "Epoch 8/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0489 - accuracy: 0.9851 - val_loss: 0.0240 - val_accuracy: 0.9948\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.99388 to 0.99477, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 9/100\n",
      "1745/1745 [==============================] - 188s 108ms/step - loss: 0.0432 - accuracy: 0.9869 - val_loss: 0.0246 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.99477\n",
      "Epoch 10/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0387 - accuracy: 0.9884 - val_loss: 0.0223 - val_accuracy: 0.9942\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.99477\n",
      "Epoch 11/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0349 - accuracy: 0.9893 - val_loss: 0.0231 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.99477\n",
      "Epoch 12/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0315 - accuracy: 0.9900 - val_loss: 0.0217 - val_accuracy: 0.9937\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.99477\n",
      "Epoch 13/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0283 - accuracy: 0.9911 - val_loss: 0.0227 - val_accuracy: 0.9942\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.99477\n",
      "Epoch 14/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0267 - accuracy: 0.9918 - val_loss: 0.0149 - val_accuracy: 0.9953\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.99477 to 0.99530, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 15/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0252 - accuracy: 0.9921 - val_loss: 0.0153 - val_accuracy: 0.9957\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.99530 to 0.99566, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 16/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0234 - accuracy: 0.9927 - val_loss: 0.0148 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.99566\n",
      "Epoch 17/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.0179 - val_accuracy: 0.9954\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.99566\n",
      "Epoch 18/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.0190 - val_accuracy: 0.9952\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.99566\n",
      "Epoch 19/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.0142 - val_accuracy: 0.9961\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.99566 to 0.99610, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 20/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.0137 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.99610\n",
      "Epoch 21/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0179 - accuracy: 0.9942 - val_loss: 0.0130 - val_accuracy: 0.9963\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.99610 to 0.99628, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 22/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0183 - accuracy: 0.9942 - val_loss: 0.0121 - val_accuracy: 0.9965\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.99628 to 0.99654, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 23/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.0154 - val_accuracy: 0.9964\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.99654\n",
      "Epoch 24/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.0133 - val_accuracy: 0.9967\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.99654 to 0.99672, saving model to ensemble/model_V1_00\\cp.ckpt\n",
      "Epoch 25/100\n",
      "1745/1745 [==============================] - 184s 105ms/step - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.0150 - val_accuracy: 0.9960\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.99672\n",
      "Epoch 26/100\n",
      " 981/1745 [===============>..............] - ETA: 1:19 - loss: 0.0135 - accuracy: 0.9959"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-81482e1917bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_path_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ensemble/model_V1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodels_V1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistories_V1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataV1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_updated_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-250d072c8c73>\u001b[0m in \u001b[0;36mtrain_models\u001b[1;34m(train, val, file_path_prefix, new_length)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{file_path_prefix}_{i:02}/cp.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         hist = model.fit(train, steps_per_epoch = new_length / BATCH_SIZE,\n\u001b[0m\u001b[0;32m     12\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                               \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\diogo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\diogo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\diogo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\diogo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\diogo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\diogo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\diogo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path_prefix = 'ensemble/model_V1'\n",
    "models_V1, histories_V1 = train_models(dataV1, valset, file_path_prefix, dataset_updated_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "close-passage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 - 24s - loss: 0.0394 - accuracy: 0.9877\n",
      "99/99 - 4s - loss: 0.0417 - accuracy: 0.9872\n",
      "99/99 - 4s - loss: 0.0392 - accuracy: 0.9892\n",
      "average accuracy: 98.804\n",
      "WARNING:tensorflow:5 out of the last 2342 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000019FEA662310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000019EB7625D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[12630, 12386, 47, 102, 17, 78, 12513, 117] 0.9907363420427553\n"
     ]
    }
   ],
   "source": [
    "load_weights(models_V1, file_path_prefix)\n",
    "evaluate_models(models_V1)\n",
    "labels_V1, logits_V1, preds_V1 = get_labels_logits_and_preds(models_V1)\n",
    "class_preds_V1 = get_class_preds(preds_V1)\n",
    "class_logits_V1 = get_class_from_sum_of_logits(logits_V1)    \n",
    "\n",
    "res = get_stats(labels_V1, class_preds_V1, class_logits_V1)\n",
    "print(res, res[6]/res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "consecutive-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[count, all_correct, all_incorrect, maj_vote, tie, maj_wrong, log_ok, log_ko], accuracy\n",
      "[12630, 12386, 47, 102, 17, 78, 12513, 117] 0.9907363420427553\n"
     ]
    }
   ],
   "source": [
    "res = get_stats(labels_V1, class_preds_V1, class_logits_V1)\n",
    "print('[count, all_correct, all_incorrect, maj_vote, tie, maj_wrong, log_ok, log_ko], accuracy')\n",
    "print(res, res[6]/res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d0dd1-2b79-4942-9f9c-4f0ae1616d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
